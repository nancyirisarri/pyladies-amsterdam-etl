apiVersion: v1
kind: ConfigMap
metadata:
  name: pyladies-amsterdam-etl-config-test
  namespace: pyladies-amsterdam-etl
data:
  # TODO Rename this to maybe MODELS.
  SOURCES: "source_1 source_2"
  # DTAP Environment
  ENVIRONMENT: "test"
  RAW_DATA_LOCAL: "/src/data/raw"
  RESULTS_DATA_LOCAL: "/src/data/results"
  # TODO Decide if can use Storage for input somehow.
  # GCP project and bucket for input raw data
  RAW_DATA_PROJECT_GCP_KEY: "/src/keys/nimellaa-test.json"
  RAW_DATA_PROJECT_GCP: "nimellaa-test"
  RAW_DATA_BUCKET_GCP: "pyladies-amsterdam-etl-data-test"
  # GCP project and bucket to upload log files
  LOG_FILES_PROJECT_GCP_KEY: "/src/keys/nimellaa-test.json"
  LOG_FILES_PROJECT_GCP: "nimellaa-test"
  LOG_FILES_BUCKET: "{}-logs-test"
  LOG_FILES_PATH: "/src/logs"
  # GCP project for this repository
  GOOGLE_CLOUD_PROJECT_KEY: "/src/keys/nimellaa-test.json"
  GOOGLE_CLOUD_PROJECT: "nimellaa-test"
  # Client specific settings
  SCRIPTS_DIR: "/src/models"
  DATASET_BIG_QUERY: "{}_DATASET_BIG_QUERY"
  SCRIPTS: "{}_SCRIPTS"
  # Source 1 Settings
  # Source 2 BigQuery dataset name
  SOURCE_1_DATASET_BIG_QUERY: "source_1"
  # List of files to process
  SOURCE_1_SCRIPTS: "source1"
  # Source 2 Settings
  # Source 2 BigQuery dataset name
  SOURCE_2_DATASET_BIG_QUERY: "source_2"
  # List of files to process
  SOURCE_2_SCRIPTS: "source2"